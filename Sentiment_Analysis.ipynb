{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis using Stanford's Amazon Fine Foods (> 5 million values) reviews and Dask\n",
    "# https://snap.stanford.edu/data/web-FineFoods.html\n",
    "\n",
    "# Main Question:\n",
    "# Can we classify the Amazon Reviews as positive or negative based on the review text? \n",
    "\n",
    "#The analysis is broken into several stages:\n",
    "# 1) Data cleaning and pre-processing\n",
    "# 2) Natural Language Processing (Tokenization) and Binary Vectorization\n",
    "# 3) Modelling (Logistic Regression, Bagged Decision Trees, Random Forests)\n",
    "# 4) Discussion\n",
    "\n",
    "# Overall - Random Forest classification was able to achieve an almost 90% classification score\n",
    "# using the top-100 tokens from the review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "######## DATA IMPORTING ########\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data was encoded uses cp1252 and not the default UTF-8\n",
    "raw_data = bag.read_text('finefoods.txt', encoding = 'cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "########## DATA CLEANING/PRE-PROCESSING ##########\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now need to find a way to add structure to the data and to \n",
    "# differentiate the different reviews - here we can use the '\\n\\n' as the delimiter.\n",
    "\n",
    "# We then parse the datafile, looking for the delimiter, but without reading the whole file into memory.\n",
    "# We will use the file iterator to look at small chunks with a set buffer size\n",
    "\n",
    "from dask.delayed import delayed\n",
    "\n",
    "def get_next_part(file, start_index, span_index = 0, blocksize = 1000):\n",
    "    file.seek(start_index)\n",
    "    buffer = file.read(blocksize + span_index).decode('cp1252') # decodes bytes to string\n",
    "    delimiter_position = buffer.find('\\n\\n')\n",
    "    if delimiter_position == -1:\n",
    "        return get_next_part(file, start_index, span_index + blocksize)\n",
    "    else:\n",
    "        file.seek(start_index)\n",
    "        return start_index, delimiter_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read through the file and use get_next_part to find each review\n",
    "\n",
    "#Output is a list of tuples - number of bytes to read after the starting byte\n",
    "\n",
    "with open('finefoods.txt', 'rb') as file_handle:\n",
    "    size = file_handle.seek(0,2) - 1 # size of the file\n",
    "    more_data = True\n",
    "    output = []\n",
    "    current_position = next_position = 0\n",
    "    while more_data:\n",
    "        if current_position >= size:\n",
    "            more_data = False\n",
    "        else:\n",
    "            current_position, next_position = get_next_part(file_handle, current_position, 0)\n",
    "            output.append((current_position, next_position)) # next position is the delimiter_position\n",
    "            current_position = current_position + next_position + 2 #delimiter is 2 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to take the review address (start byte and length of bytes) and match them to the reviews\n",
    "# Will use Dask bags to divide up the reviews to multiple workers\n",
    "# Since reviews are stored as JSON, dictionary objects for each review will be created \n",
    "\n",
    "def get_item(filename, start_index, delimiter_position, encoding = 'cp1252'):\n",
    "    with open(filename, 'rb') as file_handle:\n",
    "        file_handle.seek(start_index)\n",
    "        text = file_handle.read(delimiter_position).decode(encoding)\n",
    "        elements = text.strip().split('\\n')\n",
    "        key_value_pairs = [(element.split(': ')[0], element.split(': ')[1])\n",
    "                          if len(element.split(': ')) > 1\n",
    "                          else ('unknown', element)\n",
    "                          for element in elements]\n",
    "        return dict(key_value_pairs)\n",
    "\n",
    "\n",
    "# Combines the output addresses with the get_item function to access the reviews\n",
    "reviews = bag.from_sequence(output).map(lambda x: get_item('finefoods.txt', x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step preprocessing step before binary vectorization is to tag the\n",
    "# reviews as being either positive or negative based on review scores\n",
    "\n",
    "def tag_positive_negative_by_score(element):\n",
    "    if float(element['review/score']) > 3:\n",
    "        element['review/sentiment'] = 'positive'\n",
    "    else:\n",
    "        element['review/sentiment'] = 'negative'\n",
    "    return element\n",
    "\n",
    "tagged_reviews = reviews.map(tag_positive_negative_by_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "############### IMPLEMENTING BINARY VECTORIZATION ###############\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary vectorization - presence or absence of words (0 or 1)\n",
    "# First need to apply tooks from the NLTK for tokenization and removing stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from functools import partial\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #only words\n",
    "\n",
    "def extract_reviews(element): # isolate review text and make sure it is lower case\n",
    "    element['review/tokens'] = element['review/text'].lower()\n",
    "    return element\n",
    "\n",
    "def tokenize_reviews(element): #creates tokens from text\n",
    "    element['review/tokens'] = tokenizer.tokenize(element['review/tokens'])\n",
    "    return element\n",
    "\n",
    "def filter_stopword(word, stopwords): # filter to check for stopwords\n",
    "    return word not in stopwords\n",
    "\n",
    "def filter_stopwords(element, stopwords): #drops stopwords \n",
    "    element['review/tokens'] = list(filter(partial(filter_stopword,\n",
    "                                                 stopwords = stopwords), element['review/tokens']))\n",
    "    return element\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "more_stopwords = {'br', 'amazon', 'com', 'http', 'www', 'href', 'gp'} # adds extra stop words\n",
    "all_stopwords = stopword_set.union(more_stopwords)\n",
    "\n",
    "review_extracted_text = tagged_reviews.map(extract_reviews)\n",
    "\n",
    "review_tokens = review_extracted_text.map(tokenize_reviews)\n",
    "review_text_clean = review_tokens.map(partial(filter_stopwords, stopwords = all_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114290"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of tokens\n",
    "def extract_tokens(element):\n",
    "    return element['review/tokens']\n",
    "\n",
    "extracted_tokens = review_text_clean.map(extract_tokens)\n",
    "\n",
    "unique_tokens = extracted_tokens.flatten().distinct()\n",
    "\n",
    "number_of_tokens = unique_tokens.count().compute()\n",
    "\n",
    "number_of_tokens\n",
    "\n",
    "#We have 114290 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 21.6s\n"
     ]
    }
   ],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "#Will use the top 100 words for the binary vectorization - models could be improved with more\n",
    "# but would take substantial computing time\n",
    "\n",
    "def count(accumulator, element):\n",
    "    return accumulator + 1 #counts the instances of each word\n",
    "\n",
    "def combine(total_1, total_2):\n",
    "    return total_1 + total_2 # combines the same word across partitions\n",
    "\n",
    "with ProgressBar():\n",
    "    token_counts = extracted_tokens.flatten().foldby(lambda x: x, count, 0, combine, 0).compute()\n",
    "    top_tokens = sorted(token_counts, key = lambda x: x[1], reverse = True)\n",
    "    top_100_tokens = list(map(lambda x: x[0], top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now search through the reviews to see if they contain any of the tokens\n",
    "# by applying binary vectorization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_tokens(element):\n",
    "    vectorized_tokens = np.where(np.isin(top_100_tokens, element['review/tokens']),\n",
    "                                1, 0)\n",
    "    element['review/token_vector'] = vectorized_tokens\n",
    "    return element\n",
    "\n",
    "def prep_model_data(element):\n",
    "    return {'target': 1 if element['review/sentiment'] == 'positive' else 0,\n",
    "           'features': element['review/token_vector']}\n",
    "\n",
    "model_data = review_text_clean.map(vectorize_tokens).map(prep_model_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now need to take the bag and transform it into an array that can be used by ML\n",
    "\n",
    "from dask import array as dask_array\n",
    "\n",
    "def stacker(partition):\n",
    "    return dask_array.concatenate([element for element in partition])\n",
    "\n",
    "with ProgressBar():\n",
    "    feature_arrays = model_data.pluck('features').map(lambda x: dask_array.from_array(x, 1000).reshape(\n",
    "    1, -1)).reduction(perpartition = stacker, aggregate = stacker)\n",
    "    feature_array = feature_arrays.compute()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to save all of the above data manipulation and transformation\n",
    "# as chunks in .zarr files \n",
    "\n",
    "with ProgressBar():\n",
    "    #This saves the array\n",
    "    feature_array.rechunk(5000).to_zarr('sentiment_feature_array.zarr')\n",
    "    \n",
    "    #Calls back the feature array\n",
    "    feature_array = dask_array.from_zarr('sentiment_feature_array.zarr')\n",
    "    \n",
    "with ProgressBar():\n",
    "    target_arrays = model_data.pluck('target').map(lambda x: dask_array.from_array(x, 1000).reshape(\n",
    "    1, -1)).reduction(perpartition = stacker, aggregate = stacker)\n",
    "    \n",
    "    #Saves the target array\n",
    "    target_arrays.compute().rechunk(5000).to_zarr('sentiment_target_array.zarr')\n",
    "    \n",
    "    #Calls back the target array\n",
    "    target_array = dask_array.from_zarr('sentiment_target_array.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "############## RUNNING THE MODELS ##############\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If starting notebook after binary vectorization - can load files here\n",
    "\n",
    "from dask import array as dask_array\n",
    "\n",
    "feature_array = dask_array.from_zarr('sentiment_feature_array.zarr')\n",
    "target_array = dask_array.from_zarr('sentiment_target_array.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step is dividing the dataset in to training and testing sets\n",
    "# Train_test_split defaults to splitting 80% training and 20% testing\n",
    "\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X = feature_array\n",
    "y = target_array.flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "###### First Model - Logistic Regression ######\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we run a basic logic regression model with default parameters\n",
    "# and also use StandardScaler to scale the inputs\n",
    "# The most efficent way to do both is by using a pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "\n",
    "clf_fit = clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7957991767230764"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the test set score - we find that the basic logistic regression model\n",
    "# results in an 80% correct classification score for the reviews\n",
    "\n",
    "clf_fit.score(X_test, y_test)\n",
    "\n",
    "#This will serve as the baseline for the next set of models that we run to try and improve this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve on the baseline logistic regression model - will tune the hyperparameters using\n",
    "# GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Specifying the parameters to tune - two different penalty terms, different C values (penalty tuning parameter),\n",
    "# and three different solvers - the default lbfgs, liblinear, and saga.\n",
    "parameters = {'logisticregression__penalty': ['l1', 'l2'], \n",
    "              'logisticregression__C': [.5, 1, 2],\n",
    "             'logisticregression__solver': ['liblinear', 'saga', 'lbfgs']}\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "tuned_clf = GridSearchCV(clf, parameters) #search across all of the parameters after scaling\n",
    "\n",
    "\n",
    "tuned_clf_fit = tuned_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cv_results_ - we can see the full results of the GridSearchCV including rank_test_score\n",
    "\n",
    "#What we can see is the best estimator (also accessible as .best_estimator_) has\n",
    "# the parameters: C = 0.5, penalty = 'l1', and solver = 'saga'\n",
    "\n",
    "pd.DataFrame(tuned_clf_fit.cv_results_)\n",
    "\n",
    "tuned_best_lr = tuned_clf_fit.best_estimator_\n",
    "\n",
    "# Will then used this best_estimator to run the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7957639939485628"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results from the tuned linear regression found almost no change - essentially the same result\n",
    "# 79.57%\n",
    "tuned_best_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the model so it can be used again \n",
    "\n",
    "import dill\n",
    "\n",
    "with open('FullLRTuned.pkl', 'wb') as file:\n",
    "    dill.dump(tuned_clf_fit, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "########### SECOND MODEL - DECISION TREES ###########\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoping to improve on the classification score from the logistic model\n",
    "# we are now using a Decision Trees Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import DistanceMetric\n",
    "\n",
    "#Here there is no need to use StandardScaler() as \n",
    "# decision trees do not have the same need for standardized inputs \n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='gini', #using gini criterion\n",
    "                                   random_state = 1)\n",
    "\n",
    "tree_model_fit = tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8144460472152834"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#When we run the fitted model with the test set - we see an improved classification\n",
    "# score over what we found with the logistic regression: 81%\n",
    "\n",
    "#Not a huge improvement but is a slight one\n",
    "\n",
    "tree_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision train/test accuracies 0.979/0.814\n"
     ]
    }
   ],
   "source": [
    "# One way to examine the variance and fit of the model is to see the difference\n",
    "# in the test and train scores\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_y_train_pred = tree_model.predict(X_train)\n",
    "tree_y_test_pred = tree_model.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, tree_y_train_pred)\n",
    "tree_test = accuracy_score(y_test, tree_y_test_pred)\n",
    "\n",
    "print('Decision train/test accuracies %.3f/%.3f' %(tree_train, tree_test))\n",
    "\n",
    "#What we see is a split between train accuracy and test accuracy suggesting fairly strong\n",
    "# overfitting: 97% vs 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can attempt to adjust for the overfitting by tuning the depth of the trees\n",
    "# as well as the criterion used\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param = {'max_depth': [2, 3, 4, 5, 10, 15, 20, 30, 40],\n",
    "        'criterion': ['gini', 'entropy']}\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state = 1)\n",
    "\n",
    "tree_search = GridSearchCV(tree, param)\n",
    "\n",
    "tree_search_fit = tree_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159413151321113"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_search_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision train/test accuracies 0.923/0.816\n"
     ]
    }
   ],
   "source": [
    "#Best estimator is gini criterion with a max depth of 40\n",
    "tree_model_best = tree_search_fit.best_estimator_\n",
    "\n",
    "# Examining the train/test accuracy similar to above\n",
    "tree_y_train_pred = tree_model_best.predict(X_train)\n",
    "tree_y_test_pred = tree_model_best.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, tree_y_train_pred)\n",
    "tree_test = accuracy_score(y_test, tree_y_test_pred)\n",
    "\n",
    "print('Decision train/test accuracies %.3f/%.3f' %(tree_train, tree_test))\n",
    "\n",
    "# Tuning the parameters did not seem to make a huge difference to the \n",
    "# train/test accuracies - still appears overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One final approach to see if we can improve our fit is by using\n",
    "# bagged trees approach with bootstrap resampling\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree_bag = BaggingClassifier(base_estimator= tree_model_best, n_estimators=100, #100 trees\n",
    "                              max_samples=1.0, max_features = 1.0, bootstrap=True,\n",
    "                              bootstrap_features=False, n_jobs = -1, random_state=1)\n",
    "\n",
    "tree_bag_fit = tree_bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged decision tree train/test accuracies 0.942/0.856\n"
     ]
    }
   ],
   "source": [
    "tree_y_train_pred = tree_bag_fit.predict(X_train)\n",
    "tree_y_test_pred = tree_bag_fit.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, tree_y_train_pred)\n",
    "tree_test = accuracy_score(y_test, tree_y_test_pred)\n",
    "\n",
    "print('Bagged decision tree train/test accuracies %.3f/%.3f' %(tree_train, tree_test))\n",
    "\n",
    "#Similar to above, we compared the train/test accuracies and find a marked improvement both\n",
    "# in classification score and in the fitting. Much less overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An additional way to visualize the accuracy and bias of the model is by plotting the validation\n",
    "# curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [2, 3, 4, 5, 10, 15, 20, 30, 40]\n",
    "\n",
    "pipe_tree = make_pipeline(DecisionTreeClassifier(random_state = 1))\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    estimator = pipe_tree,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    param_name = 'decisiontreeclassifier__max_depth',\n",
    "    param_range = param_range,\n",
    "    cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_mean = np.mean(train_scores, axis = 1)\n",
    "train_std = np.std(train_scores, axis = 1)\n",
    "test_mean = np.mean(test_scores, axis = 1)\n",
    "test_std = np.std(test_scores, axis = 1)\n",
    "\n",
    "plt.plot(param_range, train_mean, color = 'blue', marker = 'o',\n",
    "        markersize = 5, label = 'Training accuracy')\n",
    "\n",
    "plt.fill_between(param_range,\n",
    "                train_mean + train_std,\n",
    "                train_mean - train_std,\n",
    "                alpha = 0.15, color = 'blue')\n",
    "\n",
    "plt.plot(param_range, test_mean,\n",
    "        color = 'green', linestyle = '--',\n",
    "        marker = 's', markersize = 5,\n",
    "        label = 'Validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, test_mean + test_std,\n",
    "                test_mean - test_std,\n",
    "                alpha = 0.15, color = 'green')\n",
    "\n",
    "plt.grid()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Parameter Max_Depth')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.ylim([0.65, 1.03])\n",
    "plt.show()\n",
    "\n",
    "#What we can see is that as the depth increases - the accuracy does not necessary\n",
    "# increase but the divide between training and test does increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current best model though is the bagged decision tree: 85% accuracy\n",
    "with open('BaggedDecisionTree.pkl', 'wb') as file:\n",
    "    dill.dump(tree_bag_fit, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############ THIRD MODEL - RANDOM FOREST ############\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  1.1s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  1.0s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  1.0s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  0.5s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# To try and further improve on the classification score of the the bagged\n",
    "# decision trees - we'll use a Random Forest classifier\n",
    "\n",
    "#setting up the parameters to be searched\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "             'max_depth': [20, 30, 40, 50]}\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25,\n",
    "                               random_state=1,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "forest_search = GridSearchCV(forest, param_grid)\n",
    "\n",
    "with ProgressBar():\n",
    "    forest_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50, n_estimators=25, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The best estimator found has a max depth of 50 and gini criterion\n",
    "forest_best = forest_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest train/test accuracies 0.958/0.864\n"
     ]
    }
   ],
   "source": [
    "forest_y_train_pred = forest_best.predict(X_train)\n",
    "forest_y_test_pred = forest_best.predict(X_test)\n",
    "\n",
    "forest_train = accuracy_score(y_train, forest_y_train_pred)\n",
    "forest_test = accuracy_score(y_test, forest_y_test_pred)\n",
    "\n",
    "print('Random Forest train/test accuracies %.3f/%.3f' %(forest_train, forest_test))\n",
    "\n",
    "# What we find when we compare the train and test accuracies is similar to what was \n",
    "# found with the bagged forest - better convergence between the test and training sets\n",
    "\n",
    "# We also see an improved classification score but not a significant improvement: 86.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling the the fit random forest model\n",
    "with open(\"Random_Forest_fit\", \"wb\") as file:\n",
    "    dill.dump(forest_best, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/Users/santiagocassalett/anaconda3/envs/py37/lib/python3.7/site-packages/dask/array/core.py:1359: FutureWarning: The `numpy.may_share_memory` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.77 (+/- 0.00) [Logistic Regression]\n",
      "ROC AUC: 0.87 (+/- 0.01) [BaggedTrees]\n",
      "ROC AUC: 0.88 (+/- 0.01) [Random Forest]\n"
     ]
    }
   ],
   "source": [
    "# Another approach for finding the best classification model for the sentiment analysis\n",
    "# is to instead run all three of the models using cross_val_score\n",
    "\n",
    "#Cross_val_score trains the models with a k-fold (here cv = 5) cross validation and\n",
    "# provides a scoring (here scoring = 'roc_auc')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf1 = LogisticRegression(solver = 'saga', penalty = 'l1', C = 0.5)\n",
    "\n",
    "clf2 = BaggingClassifier(base_estimator= tree_model_best, n_estimators=100, #100 trees\n",
    "                              max_samples=1.0, max_features = 1.0, bootstrap=True,\n",
    "                              bootstrap_features=False, n_jobs = -1, random_state=1)\n",
    "\n",
    "clf3 = RandomForestClassifier(criterion= 'gini',\n",
    "                              max_depth=50,\n",
    "                               n_estimators=25,\n",
    "                               random_state=1,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "pipe1 = Pipeline([['sc', StandardScaler()], \n",
    "                   ['lr', clf1]])\n",
    "\n",
    "clf_labels = ['Logistic Regression', 'BaggedTrees','Random Forest']\n",
    "print('5-fold cross validation')\n",
    "for clf, label in zip([pipe1, clf2, clf3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                            X = X_train,\n",
    "                            y = y_train,\n",
    "                            cv = 5,\n",
    "                            scoring = 'roc_auc')\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############## DISCUSSION ##############\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result of the above comparison between classification models shows that Random Forest has the best \n",
    "# ROC/AUC score which corresponds with the better classification score for Random Forest as well.\n",
    "\n",
    "# Overall, the use of the top-100 tokens created via binary vectorization was able to \n",
    "# correctly classify the sentiments of the amazon fine food reviews for 87-88% of the reviews.\n",
    "\n",
    "# This highlights the utility of NLP for sentiment analysis as well as the strength of random forests as a \n",
    "# classifier\n",
    "\n",
    "# Using more than the top-100 tokens (say the top-1000) could potentially improve on the current classification\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
